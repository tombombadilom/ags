# ags config files
![Screenshot](./screenshot.png)

As always a Work in Progress... 

This are my own .config/ags files, with Local Ai , Ollama and maybe later LM-Studio addons 
- Each of those projects allow LLM to be executed localy 
- For some you need GPU ,
- For others you just need CPU and an little RAM
- Finally some others need a bit of GPU , RAM and GPU

## Ollama
- [Project Web Site](https://ollama.com/)
- [Models](https://ollama.com/library)
- [Blog](https://ollama.com/blog)
- [GithHub](https://github.com/ollama/ollama)
### gguf-to-ollama
Dagger functions to import Hugging Face GGUF models into a local ollama instance and optionally push them to ollama.com.
- [GitHub](https://github.com/adrienbrault/hf-gguf-to-ollama)

## Local AI

- [Project WebSite](https://localai.io)
- [Documentation](https://localai.io/docs/)
- [GithHub](https://github.com/mudler/LocalAI)
  
## LM-Studio
- [WebSite](https://lmstudio.ai/)
- [GitHub](https://github.com/lmstudio-ai)

## UseAnything
- [WebSite](https://useanything.com/)
- [Documentation](https://docs.useanything.com/)
- [GitHub](https://github.com/Mintplex-Labs/anything-llm)

## Jan-ai
- [WebSite](https://jan.ai/)
- [Documentation](https://jan.ai/docs/)
- [GitHub](https://github.com/janhq/jan)

## AGS sources 
- [Aylur](https://github.com/Aylur)
- [Documentation](https://aylur.github.io/ags-docs/)
- [GitHub](https://github.com/Aylur/ags)

